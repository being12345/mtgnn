{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#data analysis libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# machine learning\n",
    "import sklearn\n",
    "\n",
    "#graph deep learning\n",
    "import torch\n",
    "import torch_geometric\n",
    "import networkx as nx\n",
    "\n",
    "#visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch_geometric\n",
    "%matplotlib inline\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# auto load change\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T12:48:09.880860800Z",
     "start_time": "2023-07-04T12:48:08.601707900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# summarize\n",
    "+ input_tensor: (time_num, nodes_dim * num_nodes)\n",
    "+"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# lstm train pipline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## import data\n",
    "+ metric is input tensor shape: (time_num, nodes_dim * num_nodes)\n",
    "+ every batch : `(t, b, n, c)`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-04T12:48:13.209922700Z",
     "start_time": "2023-07-04T12:48:11.883616500Z"
    }
   },
   "outputs": [],
   "source": [
    "from util.clean_data import get_and_normalize_data\n",
    "import pickle\n",
    "\n",
    "metric = get_and_normalize_data('DatasetUpdate/MMS (1).csv')\n",
    "with open('DatasetUpdate/MMS_topology.pk', 'rb') as f:\n",
    "    edge_index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "pod    adservice-76469b5944-9tlwk                                      \\\nmetric                CpuUsage(m) MemoryUsage(Mi) NetworkReceiveBytes   \n0                       -0.157415        3.169402           -0.859508   \n1                       -0.163589        3.169402           -0.860804   \n2                       -0.128732        3.169402           -0.854620   \n3                       -0.127304        3.169402           -0.860612   \n4                       -0.124751        3.169402           -0.846356   \n...                           ...             ...                 ...   \n4365                     0.048746        2.971111           -0.653714   \n4366                          NaN             NaN                 NaN   \n4367                    -0.568951        2.972818           -0.733162   \n4368                    -0.234948        2.971111           -0.723533   \n4369                    -0.031994        2.971111           -0.717625   \n\npod                                                                           \\\nmetric NetworkTransmitBytes PodLatency(s) PodSuccessRate(%) PodWorkload(Ops)   \n0                 -0.539167     -0.265056         -0.322612        -1.293704   \n1                 -0.554821     -0.265007         -0.840723        -1.259165   \n2                 -0.558797     -0.265059         -0.281728        -1.278876   \n3                 -0.559086     -0.264997         -0.935871        -1.278892   \n4                 -0.546904     -0.265005         -0.863189        -1.263901   \n...                     ...           ...               ...              ...   \n4365              -0.447064     -0.264952          0.212791        -0.500626   \n4366                    NaN           NaN               NaN              NaN   \n4367              -0.507590     -0.264851         -0.352831        -0.849824   \n4368              -0.499043     -0.264946         -0.371308        -0.756481   \n4369              -0.485635     -0.265043          0.049564        -0.765983   \n\npod    adservice-76469b5944-h6cqg                                      ...  \\\nmetric                CpuUsage(m) MemoryUsage(Mi) NetworkReceiveBytes  ...   \n0                       -0.250438        3.018257           -0.862908  ...   \n1                       -0.408388        3.018044           -0.862873  ...   \n2                       -0.293421        3.018257           -0.864335  ...   \n3                       -0.244242        3.018257           -0.862536  ...   \n4                       -0.227894        3.018044           -0.858572  ...   \n...                           ...             ...                 ...  ...   \n4365                          NaN             NaN                 NaN  ...   \n4366                     0.020721        3.200441           -0.655733  ...   \n4367                    -0.367106        3.200654           -0.733405  ...   \n4368                    -0.106638        3.200441           -0.712772  ...   \n4369                    -0.373853        3.200441           -0.715206  ...   \n\npod    shippingservice-7499794f94-gmcld                                     \\\nmetric                    PodLatency(s) PodSuccessRate(%) PodWorkload(Ops)   \n0                             -0.252401          0.372415        -1.239457   \n1                             -0.263458          0.372415        -1.254233   \n2                             -0.231213          0.372415        -1.308469   \n3                             -0.237058          0.372415        -1.343006   \n4                             -0.259269          0.372415        -1.313256   \n...                                 ...               ...              ...   \n4365                                NaN               NaN              NaN   \n4366                          -0.251305          0.372415        -0.594243   \n4367                          -0.252714          0.372415        -0.829910   \n4368                          -0.248383          0.372415        -0.855071   \n4369                          -0.250209          0.372415        -0.741324   \n\npod    shippingservice-7499794f94-s67q7                                      \\\nmetric                      CpuUsage(m) MemoryUsage(Mi) NetworkReceiveBytes   \n0                             -1.059918       -0.635869           -0.844217   \n1                             -1.064412       -0.635869           -0.843862   \n2                             -1.049463       -0.635869           -0.851999   \n3                             -1.054402       -0.635869           -0.856789   \n4                             -1.045631       -0.635869           -0.853883   \n...                                 ...             ...                 ...   \n4365                          -0.960991       -0.634056           -0.659874   \n4366                                NaN             NaN                 NaN   \n4367                          -0.908202       -0.634056           -0.715682   \n4368                          -0.982859       -0.634056           -0.744884   \n4369                          -0.901357       -0.634056           -0.695597   \n\npod                                                                           \nmetric NetworkTransmitBytes PodLatency(s) PodSuccessRate(%) PodWorkload(Ops)  \n0                 -0.560786     -0.236014          0.372415        -1.214800  \n1                 -0.564429     -0.250940          0.372415        -1.264097  \n2                 -0.567312     -0.258392          0.372415        -1.303536  \n3                 -0.569501     -0.264869          0.372415        -1.298620  \n4                 -0.563497     -0.244364          0.372415        -1.293514  \n...                     ...           ...               ...              ...  \n4365              -0.471441     -0.252088          0.372415        -0.618879  \n4366                    NaN           NaN               NaN              NaN  \n4367              -0.490145     -0.239981          0.372415        -0.810177  \n4368              -0.507500     -0.235402          0.372415        -0.869859  \n4369              -0.506083     -0.248261          0.372415        -0.790691  \n\n[4370 rows x 350 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th>pod</th>\n      <th colspan=\"7\" halign=\"left\">adservice-76469b5944-9tlwk</th>\n      <th colspan=\"3\" halign=\"left\">adservice-76469b5944-h6cqg</th>\n      <th>...</th>\n      <th colspan=\"3\" halign=\"left\">shippingservice-7499794f94-gmcld</th>\n      <th colspan=\"7\" halign=\"left\">shippingservice-7499794f94-s67q7</th>\n    </tr>\n    <tr>\n      <th>metric</th>\n      <th>CpuUsage(m)</th>\n      <th>MemoryUsage(Mi)</th>\n      <th>NetworkReceiveBytes</th>\n      <th>NetworkTransmitBytes</th>\n      <th>PodLatency(s)</th>\n      <th>PodSuccessRate(%)</th>\n      <th>PodWorkload(Ops)</th>\n      <th>CpuUsage(m)</th>\n      <th>MemoryUsage(Mi)</th>\n      <th>NetworkReceiveBytes</th>\n      <th>...</th>\n      <th>PodLatency(s)</th>\n      <th>PodSuccessRate(%)</th>\n      <th>PodWorkload(Ops)</th>\n      <th>CpuUsage(m)</th>\n      <th>MemoryUsage(Mi)</th>\n      <th>NetworkReceiveBytes</th>\n      <th>NetworkTransmitBytes</th>\n      <th>PodLatency(s)</th>\n      <th>PodSuccessRate(%)</th>\n      <th>PodWorkload(Ops)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.157415</td>\n      <td>3.169402</td>\n      <td>-0.859508</td>\n      <td>-0.539167</td>\n      <td>-0.265056</td>\n      <td>-0.322612</td>\n      <td>-1.293704</td>\n      <td>-0.250438</td>\n      <td>3.018257</td>\n      <td>-0.862908</td>\n      <td>...</td>\n      <td>-0.252401</td>\n      <td>0.372415</td>\n      <td>-1.239457</td>\n      <td>-1.059918</td>\n      <td>-0.635869</td>\n      <td>-0.844217</td>\n      <td>-0.560786</td>\n      <td>-0.236014</td>\n      <td>0.372415</td>\n      <td>-1.214800</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.163589</td>\n      <td>3.169402</td>\n      <td>-0.860804</td>\n      <td>-0.554821</td>\n      <td>-0.265007</td>\n      <td>-0.840723</td>\n      <td>-1.259165</td>\n      <td>-0.408388</td>\n      <td>3.018044</td>\n      <td>-0.862873</td>\n      <td>...</td>\n      <td>-0.263458</td>\n      <td>0.372415</td>\n      <td>-1.254233</td>\n      <td>-1.064412</td>\n      <td>-0.635869</td>\n      <td>-0.843862</td>\n      <td>-0.564429</td>\n      <td>-0.250940</td>\n      <td>0.372415</td>\n      <td>-1.264097</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.128732</td>\n      <td>3.169402</td>\n      <td>-0.854620</td>\n      <td>-0.558797</td>\n      <td>-0.265059</td>\n      <td>-0.281728</td>\n      <td>-1.278876</td>\n      <td>-0.293421</td>\n      <td>3.018257</td>\n      <td>-0.864335</td>\n      <td>...</td>\n      <td>-0.231213</td>\n      <td>0.372415</td>\n      <td>-1.308469</td>\n      <td>-1.049463</td>\n      <td>-0.635869</td>\n      <td>-0.851999</td>\n      <td>-0.567312</td>\n      <td>-0.258392</td>\n      <td>0.372415</td>\n      <td>-1.303536</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.127304</td>\n      <td>3.169402</td>\n      <td>-0.860612</td>\n      <td>-0.559086</td>\n      <td>-0.264997</td>\n      <td>-0.935871</td>\n      <td>-1.278892</td>\n      <td>-0.244242</td>\n      <td>3.018257</td>\n      <td>-0.862536</td>\n      <td>...</td>\n      <td>-0.237058</td>\n      <td>0.372415</td>\n      <td>-1.343006</td>\n      <td>-1.054402</td>\n      <td>-0.635869</td>\n      <td>-0.856789</td>\n      <td>-0.569501</td>\n      <td>-0.264869</td>\n      <td>0.372415</td>\n      <td>-1.298620</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.124751</td>\n      <td>3.169402</td>\n      <td>-0.846356</td>\n      <td>-0.546904</td>\n      <td>-0.265005</td>\n      <td>-0.863189</td>\n      <td>-1.263901</td>\n      <td>-0.227894</td>\n      <td>3.018044</td>\n      <td>-0.858572</td>\n      <td>...</td>\n      <td>-0.259269</td>\n      <td>0.372415</td>\n      <td>-1.313256</td>\n      <td>-1.045631</td>\n      <td>-0.635869</td>\n      <td>-0.853883</td>\n      <td>-0.563497</td>\n      <td>-0.244364</td>\n      <td>0.372415</td>\n      <td>-1.293514</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4365</th>\n      <td>0.048746</td>\n      <td>2.971111</td>\n      <td>-0.653714</td>\n      <td>-0.447064</td>\n      <td>-0.264952</td>\n      <td>0.212791</td>\n      <td>-0.500626</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-0.960991</td>\n      <td>-0.634056</td>\n      <td>-0.659874</td>\n      <td>-0.471441</td>\n      <td>-0.252088</td>\n      <td>0.372415</td>\n      <td>-0.618879</td>\n    </tr>\n    <tr>\n      <th>4366</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.020721</td>\n      <td>3.200441</td>\n      <td>-0.655733</td>\n      <td>...</td>\n      <td>-0.251305</td>\n      <td>0.372415</td>\n      <td>-0.594243</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4367</th>\n      <td>-0.568951</td>\n      <td>2.972818</td>\n      <td>-0.733162</td>\n      <td>-0.507590</td>\n      <td>-0.264851</td>\n      <td>-0.352831</td>\n      <td>-0.849824</td>\n      <td>-0.367106</td>\n      <td>3.200654</td>\n      <td>-0.733405</td>\n      <td>...</td>\n      <td>-0.252714</td>\n      <td>0.372415</td>\n      <td>-0.829910</td>\n      <td>-0.908202</td>\n      <td>-0.634056</td>\n      <td>-0.715682</td>\n      <td>-0.490145</td>\n      <td>-0.239981</td>\n      <td>0.372415</td>\n      <td>-0.810177</td>\n    </tr>\n    <tr>\n      <th>4368</th>\n      <td>-0.234948</td>\n      <td>2.971111</td>\n      <td>-0.723533</td>\n      <td>-0.499043</td>\n      <td>-0.264946</td>\n      <td>-0.371308</td>\n      <td>-0.756481</td>\n      <td>-0.106638</td>\n      <td>3.200441</td>\n      <td>-0.712772</td>\n      <td>...</td>\n      <td>-0.248383</td>\n      <td>0.372415</td>\n      <td>-0.855071</td>\n      <td>-0.982859</td>\n      <td>-0.634056</td>\n      <td>-0.744884</td>\n      <td>-0.507500</td>\n      <td>-0.235402</td>\n      <td>0.372415</td>\n      <td>-0.869859</td>\n    </tr>\n    <tr>\n      <th>4369</th>\n      <td>-0.031994</td>\n      <td>2.971111</td>\n      <td>-0.717625</td>\n      <td>-0.485635</td>\n      <td>-0.265043</td>\n      <td>0.049564</td>\n      <td>-0.765983</td>\n      <td>-0.373853</td>\n      <td>3.200441</td>\n      <td>-0.715206</td>\n      <td>...</td>\n      <td>-0.250209</td>\n      <td>0.372415</td>\n      <td>-0.741324</td>\n      <td>-0.901357</td>\n      <td>-0.634056</td>\n      <td>-0.695597</td>\n      <td>-0.506083</td>\n      <td>-0.248261</td>\n      <td>0.372415</td>\n      <td>-0.790691</td>\n    </tr>\n  </tbody>\n</table>\n<p>4370 rows × 350 columns</p>\n</div>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T12:48:16.979823600Z",
     "start_time": "2023-07-04T12:48:16.346804800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## split train and test, then train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "  0%|          | 0/66 [00:00<?, ?it/s]\u001B[A\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m train, test \u001B[38;5;241m=\u001B[39m train_test_split(metric, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.3\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m      6\u001B[0m lstm \u001B[38;5;241m=\u001B[39m GraphLSTM_VAE_AD(variational\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m----> 7\u001B[0m \u001B[43mlstm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\TopoMAD\\graph_lstm_vae_ad_ver6\\graphlstm_vae_ad.py:160\u001B[0m, in \u001B[0;36mGraphLSTM_VAE_AD.fit\u001B[1;34m(self, X, nodes_num, edge_index, log_step, patience, selected_indexes)\u001B[0m\n\u001B[0;32m    158\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m (i, ts_batch) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(tqdm(train_loader)):\n\u001B[0;32m    159\u001B[0m     use_teacher_forcing \u001B[38;5;241m=\u001B[39m random\u001B[38;5;241m.\u001B[39mrandom() \u001B[38;5;241m<\u001B[39m teacher_forcing_ratio\n\u001B[1;32m--> 160\u001B[0m     output, enc_hidden \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstmed\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_var\u001B[49m\u001B[43m(\u001B[49m\u001B[43mts_batch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_teacher_forcing\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    161\u001B[0m     total_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlstmed\u001B[38;5;241m.\u001B[39mloss_function(output, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mto_var(ts_batch\u001B[38;5;241m.\u001B[39mfloat()))\n\u001B[0;32m    163\u001B[0m     loss \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[1;32mD:\\conda\\envs\\kaggle\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\Desktop\\TopoMAD\\graph_lstm_vae_ad_ver6\\graphlstm_vae.py:95\u001B[0m, in \u001B[0;36mGraphLSTM_VAE.forward\u001B[1;34m(self, ts_batch, edge_index, use_teacher_forcing)\u001B[0m\n\u001B[0;32m     92\u001B[0m     ts_batch \u001B[38;5;241m=\u001B[39m ts_batch\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m)\u001B[38;5;241m.\u001B[39mcontiguous()\n\u001B[0;32m     94\u001B[0m \u001B[38;5;66;03m# 1. Encode the timeseries to make use of the last hidden state.\u001B[39;00m\n\u001B[1;32m---> 95\u001B[0m _, enc_hidden0 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mts_batch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# .float() here or .double() for the model\u001B[39;00m\n\u001B[0;32m     97\u001B[0m enc_hidden \u001B[38;5;241m=\u001B[39m enc_hidden0[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     98\u001B[0m \u001B[38;5;66;03m# 2. Use hidden state as initialization for our Decoder-LSTM\u001B[39;00m\n",
      "File \u001B[1;32mD:\\conda\\envs\\kaggle\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\Desktop\\TopoMAD\\graph_lstm_vae_ad_ver6\\graphlstm.py:365\u001B[0m, in \u001B[0;36mGraphLSTM.forward\u001B[1;34m(self, input_tensor, edge_index, hidden_state)\u001B[0m\n\u001B[0;32m    363\u001B[0m output_inner \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    364\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(seq_len):\n\u001B[1;32m--> 365\u001B[0m     h, c \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcell_list\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlayer_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_tensor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcur_layer_input\u001B[49m\u001B[43m[\u001B[49m\u001B[43mt\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    366\u001B[0m \u001B[43m                                     \u001B[49m\u001B[43medge_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43medge_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcur_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    367\u001B[0m     output_inner\u001B[38;5;241m.\u001B[39mappend(h)\n\u001B[0;32m    369\u001B[0m layer_output \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack(output_inner, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32mD:\\conda\\envs\\kaggle\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\Desktop\\TopoMAD\\graph_lstm_vae_ad_ver6\\graphlstm.py:51\u001B[0m, in \u001B[0;36mGCNLSTMCell.forward\u001B[1;34m(self, input_tensor, cur_state, edge_index)\u001B[0m\n\u001B[0;32m     48\u001B[0m h_cur, c_cur \u001B[38;5;241m=\u001B[39m cur_state\n\u001B[0;32m     50\u001B[0m combined \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([input_tensor, h_cur], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)  \u001B[38;5;66;03m# concatenate along hidden axis\u001B[39;00m\n\u001B[1;32m---> 51\u001B[0m batch \u001B[38;5;241m=\u001B[39m \u001B[43mBatch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_data_list\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mData\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcombined\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43medge_index\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mrange\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcombined\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     53\u001B[0m combined_conv \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgconv(batch\u001B[38;5;241m.\u001B[39mx, batch\u001B[38;5;241m.\u001B[39medge_index)\n\u001B[0;32m     54\u001B[0m combined_conv \u001B[38;5;241m=\u001B[39m combined_conv\u001B[38;5;241m.\u001B[39mreshape(combined\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], combined\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32mD:\\conda\\envs\\kaggle\\lib\\site-packages\\torch_geometric\\data\\batch.py:76\u001B[0m, in \u001B[0;36mBatch.from_data_list\u001B[1;34m(cls, data_list, follow_batch, exclude_keys)\u001B[0m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfrom_data_list\u001B[39m(\u001B[38;5;28mcls\u001B[39m, data_list: List[BaseData],\n\u001B[0;32m     66\u001B[0m                    follow_batch: Optional[List[\u001B[38;5;28mstr\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     67\u001B[0m                    exclude_keys: Optional[List[\u001B[38;5;28mstr\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m     68\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001B[39;00m\n\u001B[0;32m     69\u001B[0m \u001B[38;5;124;03m    Python list of :class:`~torch_geometric.data.Data` or\u001B[39;00m\n\u001B[0;32m     70\u001B[0m \u001B[38;5;124;03m    :class:`~torch_geometric.data.HeteroData` objects.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     73\u001B[0m \u001B[38;5;124;03m    :obj:`follow_batch`.\u001B[39;00m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;124;03m    Will exclude any keys given in :obj:`exclude_keys`.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 76\u001B[0m     batch, slice_dict, inc_dict \u001B[38;5;241m=\u001B[39m \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     77\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     78\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata_list\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_list\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     79\u001B[0m \u001B[43m        \u001B[49m\u001B[43mincrement\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     80\u001B[0m \u001B[43m        \u001B[49m\u001B[43madd_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata_list\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mBatch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     81\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfollow_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_batch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     82\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexclude_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexclude_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     83\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     85\u001B[0m     batch\u001B[38;5;241m.\u001B[39m_num_graphs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(data_list)\n\u001B[0;32m     86\u001B[0m     batch\u001B[38;5;241m.\u001B[39m_slice_dict \u001B[38;5;241m=\u001B[39m slice_dict\n",
      "File \u001B[1;32mD:\\conda\\envs\\kaggle\\lib\\site-packages\\torch_geometric\\data\\collate.py:109\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001B[0m\n\u001B[0;32m    106\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (add_batch \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(stores[\u001B[38;5;241m0\u001B[39m], NodeStorage)\n\u001B[0;32m    107\u001B[0m             \u001B[38;5;129;01mand\u001B[39;00m stores[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mcan_infer_num_nodes):\n\u001B[0;32m    108\u001B[0m         repeats \u001B[38;5;241m=\u001B[39m [store\u001B[38;5;241m.\u001B[39mnum_nodes \u001B[38;5;28;01mfor\u001B[39;00m store \u001B[38;5;129;01min\u001B[39;00m stores]\n\u001B[1;32m--> 109\u001B[0m         out_store\u001B[38;5;241m.\u001B[39mbatch \u001B[38;5;241m=\u001B[39m \u001B[43mrepeat_interleave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrepeats\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    110\u001B[0m         out_store\u001B[38;5;241m.\u001B[39mptr \u001B[38;5;241m=\u001B[39m cumsum(torch\u001B[38;5;241m.\u001B[39mtensor(repeats, device\u001B[38;5;241m=\u001B[39mdevice))\n\u001B[0;32m    112\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out, slice_dict, inc_dict\n",
      "File \u001B[1;32mD:\\conda\\envs\\kaggle\\lib\\site-packages\\torch_geometric\\data\\collate.py:252\u001B[0m, in \u001B[0;36mrepeat_interleave\u001B[1;34m(repeats, device)\u001B[0m\n\u001B[0;32m    248\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrepeat_interleave\u001B[39m(\n\u001B[0;32m    249\u001B[0m     repeats: List[\u001B[38;5;28mint\u001B[39m],\n\u001B[0;32m    250\u001B[0m     device: Optional[torch\u001B[38;5;241m.\u001B[39mdevice] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    251\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 252\u001B[0m     outs \u001B[38;5;241m=\u001B[39m [torch\u001B[38;5;241m.\u001B[39mfull((n, ), i, device\u001B[38;5;241m=\u001B[39mdevice) \u001B[38;5;28;01mfor\u001B[39;00m i, n \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(repeats)]\n\u001B[0;32m    253\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcat(outs, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32mD:\\conda\\envs\\kaggle\\lib\\site-packages\\torch_geometric\\data\\collate.py:252\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    248\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrepeat_interleave\u001B[39m(\n\u001B[0;32m    249\u001B[0m     repeats: List[\u001B[38;5;28mint\u001B[39m],\n\u001B[0;32m    250\u001B[0m     device: Optional[torch\u001B[38;5;241m.\u001B[39mdevice] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    251\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 252\u001B[0m     outs \u001B[38;5;241m=\u001B[39m [\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfull\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m i, n \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(repeats)]\n\u001B[0;32m    253\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcat(outs, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from graph_lstm_vae_ad_ver6 import GraphLSTM_VAE_AD\n",
    "\n",
    "train, test = train_test_split(metric, test_size=0.3, shuffle=False)\n",
    "\n",
    "lstm = GraphLSTM_VAE_AD(variational=False)\n",
    "lstm.fit(train, 50, edge_index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T11:20:56.397555800Z",
     "start_time": "2023-07-04T11:20:52.215713100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## important process and data\n",
    "In this section, I'll explain the architecture of this nn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### set sequence for train and test\n",
    "+ train: 0.7 valid: 0.3\n",
    "+ sequence shape: `(batch_size, sequence_length(known as timestep), nodes_num, performance_metrics_num)` in this table is (4341, 30, 50, 7)\n",
    "\n",
    "in `graphlstm_vae_ad.py`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# sequence = lstm.get_time_sequence(metric.values, 50)\n",
    "#\n",
    "# from torch.utils.data import DataLoader, random_split\n",
    "#\n",
    "# trainset, validset = random_split(sequence, [0.7, 0.3])\n",
    "# train_loader = DataLoader(dataset=trainset, batch_size=32, drop_last=True, shuffle=True, pin_memory=False)\n",
    "# valid_loader = DataLoader(dataset=validset, batch_size=32, drop_last=True, shuffle=True, pin_memory=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T11:25:26.418279700Z",
     "start_time": "2023-07-04T11:25:26.003056600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### graphlstm_vae architecture\n",
    "+ set encoder and decoder layer in `num_layers: tuple = (2, 2)`\n",
    "+ every batch : `(t, b, n, c)`\n",
    "+ set hidden dim  in `num_layers: int` but in graphlstm unit it transform to list `param = [param] * num_layers`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/94 [00:01<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from graph_lstm_vae_ad_ver6 import GraphLSTM_VAE_AD\n",
    "\n",
    "module = GraphLSTM_VAE_AD(num_layers=(3, 2), hidden_dim=6)\n",
    "GraphLSTM_VAE = module.fit(metric, 50, edge_index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T10:38:30.346866300Z",
     "start_time": "2023-07-04T10:38:28.488711400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## encoder\n",
    "compose of `self.num_layers[0]` conv nn\n",
    "To use the encoder module, follow following shape\n",
    "+  `ts_batch`: `(t, b, n, c)`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "GraphLSTM(\n  (cell_list): ModuleList(\n    (0): GCNLSTMCell(\n      (gconv): GCNConv(12, 20)\n    )\n    (1): GCNLSTMCell(\n      (gconv): GCNConv(10, 20)\n    )\n  )\n)"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoder module"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T05:42:55.386826100Z",
     "start_time": "2023-07-04T05:42:55.141822100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GraphLSTM unit\n",
    "+ h, c shape: (batch_size, num_nodes, num_hidden), (batch_size, num_nodes, num_hidden)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "# call GraphLSTM unit forward will return h_next, c_next"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T07:43:22.465254100Z",
     "start_time": "2023-07-04T07:43:22.227523200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### encoder output\n",
    "call `_, enc_hidden0 = self.encoder(ts_batch.float(), edge_index)` get state\n",
    "\n",
    "following code will generate encoder output:\n",
    "+ layer_output_list shape: `(num_layers,batch_size, num_nodes, num_hidden)` h in this\n",
    "+ layer_state_list shape: `(num_layers, [(batch_size, num_nodes, num_hidden), (batch_size, batch_size, num_nodes, num_hidden)])` (h, c) in this"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for layer_idx in range(self.num_layers):\n",
    "#\n",
    "#     h, c = hidden_state[layer_idx]\n",
    "#     output_inner = []\n",
    "#     for t in range(seq_len):\n",
    "#         h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[t],\n",
    "#                                          edge_index=edge_index, cur_state=[h, c])\n",
    "#         output_inner.append(h)\n",
    "#\n",
    "#     layer_output = torch.stack(output_inner, dim=0)\n",
    "#     cur_layer_input = layer_output  # 4 dim\n",
    "#\n",
    "#     layer_output_list.append(layer_output)\n",
    "#     last_state_list.append([h, c])\n",
    "#\n",
    "# if not self.return_all_layers:\n",
    "#     layer_output_list = layer_output_list[-1:]\n",
    "#     last_state_list = last_state_list[-1:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## decoder without variational\n",
    "+ first output shape: `(time_step, batch_size, num_nodes, nodes_dim)` in this table is (30, 32, 50, 7)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-0.8908,  0.5737, -1.1470,  0.0472,  0.6765,  0.3537, -0.1155],\n       grad_fn=<SelectBackward0>)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get last output using nn.linear\n",
    "# output[ts_batch.shape[0] - 1] = self.deal_list(self.hidden2output,\n",
    "#                                                enc_hidden)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T10:34:30.269278100Z",
     "start_time": "2023-07-04T10:34:29.804271200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### two way to train\n",
    "1. using last output as input\n",
    "2. using this time batch as input\n",
    "+ `hidden_state` shape: `[[(b, n, h), (b, n, h)]] * num_layers` using encoder state as decoder hidden_state\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for i in reversed(range(ts_batch.shape[0] - 1)):\n",
    "#     if self.training and use_teacher_forcing:\n",
    "#         _, dec_hidden = self.decoder(ts_batch[i + 1].unsqueeze(0).float(), edge_index, dec_hidden)\n",
    "#     else:\n",
    "#         _, dec_hidden = self.decoder(output[i + 1].unsqueeze(0), edge_index, dec_hidden)\n",
    "#     output[i] = self.deal_list(self.hidden2output, dec_hidden[-1][\n",
    "#         0])  # self.hidden2output(dec_hidden[-1][0])#self.deal_batch(self.hidden2output, dec_hidden[-1][0], edge_index)#self.hidden2output(dec_hidden[-1][0], edge_index)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
